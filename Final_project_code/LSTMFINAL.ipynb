{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "681d2230-33c4-4c0d-8021-33c3c2a84b22",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_code_encoded_index</th>\n",
       "      <th>SQLDATE</th>\n",
       "      <th>EventCode</th>\n",
       "      <th>QuadClass</th>\n",
       "      <th>GoldsteinScale</th>\n",
       "      <th>AvgTone</th>\n",
       "      <th>NumArticles</th>\n",
       "      <th>NumMentions</th>\n",
       "      <th>Contains14sub</th>\n",
       "      <th>Contains14</th>\n",
       "      <th>country_code_encoded</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>DayOfMonth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2015-02-19</td>\n",
       "      <td>141</td>\n",
       "      <td>3</td>\n",
       "      <td>-6.5</td>\n",
       "      <td>-3.543307</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2015-02-20</td>\n",
       "      <td>141</td>\n",
       "      <td>3</td>\n",
       "      <td>-6.5</td>\n",
       "      <td>-3.543307</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2015-02-21</td>\n",
       "      <td>141</td>\n",
       "      <td>3</td>\n",
       "      <td>-6.5</td>\n",
       "      <td>-2.558272</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2015-02-22</td>\n",
       "      <td>141</td>\n",
       "      <td>3</td>\n",
       "      <td>-6.5</td>\n",
       "      <td>-8.896797</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2015-02-23</td>\n",
       "      <td>141</td>\n",
       "      <td>3</td>\n",
       "      <td>-6.5</td>\n",
       "      <td>-8.160237</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   country_code_encoded_index     SQLDATE  EventCode  QuadClass  \\\n",
       "0                           0  2015-02-19        141          3   \n",
       "1                           0  2015-02-20        141          3   \n",
       "2                           0  2015-02-21        141          3   \n",
       "3                           0  2015-02-22        141          3   \n",
       "4                           0  2015-02-23        141          3   \n",
       "\n",
       "   GoldsteinScale   AvgTone  NumArticles  NumMentions  Contains14sub  \\\n",
       "0            -6.5 -3.543307            1            1              1   \n",
       "1            -6.5 -3.543307            1            1              1   \n",
       "2            -6.5 -2.558272           10           10              1   \n",
       "3            -6.5 -8.896797           10           10              1   \n",
       "4            -6.5 -8.160237           10           10              1   \n",
       "\n",
       "   Contains14  country_code_encoded  Year  Month  DayOfMonth  \n",
       "0           0                     0  2015      2          19  \n",
       "1           0                     0  2015      2          20  \n",
       "2           0                     0  2015      2          21  \n",
       "3           0                     0  2015      2          22  \n",
       "4           0                     0  2015      2          23  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Load the data\n",
    "data = pd.read_csv('TEST')\n",
    "# data = pd.read_csv('events_HK_processed_18.csv')\n",
    "# data = pd.read_csv('events_HK_2015_2020_test.csv')\n",
    "# Display the first few rows of the dataframe\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0991ae66-d7fe-4600-ae96-78e3c1aa9d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data.drop(columns=['Contains14sub', 'ActionGeo_CountryCode', 'EventCode'])\n",
    "# data.rename(columns={'Contains14': 'lag_fut_Contains14 1'}, inplace=True)\n",
    "# data['AvgTone'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a3522b1-5928-4bcd-8d3c-227f9fefecb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['Contains14sub', 'EventCode', 'country_code_encoded_index', 'country_code_encoded', 'Year', 'Month', 'DayOfMonth', 'SQLDATE'])\n",
    "data.rename(columns={'Contains14': 'lag_fut_Contains14 1'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d640997e-a632-450c-adea-10c61d49e2bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lag_fut_Contains14 1\n",
       "0    1720\n",
       "1      57\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.value_counts('lag_fut_Contains14 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "058fbbec-c4fd-4f51-8c94-2bba7132ad8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  define a window in the future to predict\n",
    "Days_in_future = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8055f70-f873-43b0-88bd-6aa14ca3aa70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SQLDATE</th>\n",
       "      <th>QuadClass</th>\n",
       "      <th>GoldsteinScale</th>\n",
       "      <th>AvgTone</th>\n",
       "      <th>NumArticles</th>\n",
       "      <th>NumMentions</th>\n",
       "      <th>lag_fut_Contains14 1</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>DayOfMonth</th>\n",
       "      <th>will_be_1_anytime_next_2_days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-02-19</td>\n",
       "      <td>3</td>\n",
       "      <td>-6.5</td>\n",
       "      <td>-3.543307</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-02-20</td>\n",
       "      <td>3</td>\n",
       "      <td>-6.5</td>\n",
       "      <td>-3.543307</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-02-21</td>\n",
       "      <td>3</td>\n",
       "      <td>-6.5</td>\n",
       "      <td>-2.558272</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-02-22</td>\n",
       "      <td>3</td>\n",
       "      <td>-6.5</td>\n",
       "      <td>-8.896797</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-02-23</td>\n",
       "      <td>3</td>\n",
       "      <td>-6.5</td>\n",
       "      <td>-8.160237</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2015-02-24</td>\n",
       "      <td>3</td>\n",
       "      <td>-6.5</td>\n",
       "      <td>0.471698</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2015-02-25</td>\n",
       "      <td>3</td>\n",
       "      <td>-6.5</td>\n",
       "      <td>0.155521</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2015-02-26</td>\n",
       "      <td>3</td>\n",
       "      <td>-6.5</td>\n",
       "      <td>-0.838574</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2015-02-27</td>\n",
       "      <td>3</td>\n",
       "      <td>-6.5</td>\n",
       "      <td>-9.756098</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2015-02-28</td>\n",
       "      <td>3</td>\n",
       "      <td>-6.5</td>\n",
       "      <td>-9.756098</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      SQLDATE  QuadClass  GoldsteinScale   AvgTone  NumArticles  NumMentions  \\\n",
       "0  2015-02-19          3            -6.5 -3.543307            1            1   \n",
       "1  2015-02-20          3            -6.5 -3.543307            1            1   \n",
       "2  2015-02-21          3            -6.5 -2.558272           10           10   \n",
       "3  2015-02-22          3            -6.5 -8.896797           10           10   \n",
       "4  2015-02-23          3            -6.5 -8.160237           10           10   \n",
       "5  2015-02-24          3            -6.5  0.471698            8            8   \n",
       "6  2015-02-25          3            -6.5  0.155521           10           10   \n",
       "7  2015-02-26          3            -6.5 -0.838574            2            2   \n",
       "8  2015-02-27          3            -6.5 -9.756098            3            3   \n",
       "9  2015-02-28          3            -6.5 -9.756098            3            3   \n",
       "\n",
       "   lag_fut_Contains14 1  Year  Month  DayOfMonth  \\\n",
       "0                     0  2015      2          19   \n",
       "1                     0  2015      2          20   \n",
       "2                     0  2015      2          21   \n",
       "3                     0  2015      2          22   \n",
       "4                     0  2015      2          23   \n",
       "5                     0  2015      2          24   \n",
       "6                     0  2015      2          25   \n",
       "7                     0  2015      2          26   \n",
       "8                     0  2015      2          27   \n",
       "9                     0  2015      2          28   \n",
       "\n",
       "   will_be_1_anytime_next_2_days  \n",
       "0                              0  \n",
       "1                              0  \n",
       "2                              0  \n",
       "3                              0  \n",
       "4                              0  \n",
       "5                              0  \n",
       "6                              0  \n",
       "7                              0  \n",
       "8                              0  \n",
       "9                              0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "target_name = 'will_be_1_anytime_next_'+str(Days_in_future)+'_days'\n",
    "# Create a rolling window of 7 days and check if there's any 1 in 'lag_fut_Contains14 1' column within this window\n",
    "data[target_name] = data['lag_fut_Contains14 1'].rolling(window=Days_in_future, min_periods=1).apply(lambda x: 1 if x.sum() > 0 else 0)\n",
    "\n",
    "# Shift the 'will_be_1_anytime_next_week' column up by 1 place\n",
    "data[target_name] = data[target_name].shift(-1)\n",
    "\n",
    "# Replace NaN values with 0\n",
    "# data[target_name].fillna(0, inplace=True)\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "\n",
    "# Convert the new column to integers\n",
    "data[target_name] = data[target_name].astype(int)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15798f19-2151-493a-84be-7cc8273e776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(data.index,data['lag_fut_Contains14 1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3df14cf-cff4-4467-a10a-adf7e5a231b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define lookback and buffer window\n",
    "#  Units are number of days\n",
    "window_gap = 0\n",
    "lookback = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e19665a0-bcd9-4881-92f3-eb54963b32e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling(df, lookback_lag_target, roll_range):\n",
    "    df_lag = df.copy()\n",
    "    # Create lagged features\n",
    "    for lag in lookback_lag_target:\n",
    "        df_lag[f'lag_{lag}'] = df_lag[target_name].shift(lag)\n",
    "        # df[f'lagGoldstein_{lag}'] = df['GoldsteinScale'].shift(lag)\n",
    "        # df[f'lagAvgTone_{lag}'] = df['AvgTone'].shift(lag)\n",
    "        # df[f'lagAvgTone_{9}'] = df['AvgTone'].shift(9)\n",
    "        # df[f'lagNumArticles_{lag}'] = df['NumArticles'].shift(lag)\n",
    "        # df[f'lagNumMentions_{lag}'] = df['NumMentions'].shift(lag)\n",
    "        # df[f'lagQuadClass_{lag}'] = df['QuadClass'].shift(lag)\n",
    "#     drop columns that refer to same day data\n",
    "    \n",
    "    columns_to_drop = ['GoldsteinScale', 'AvgTone', 'NumArticles', 'NumMentions', 'QuadClass', 'lag_fut_Contains14 1']\n",
    "    df_removed=df_lag.copy()\n",
    "    df_removed = df_removed.drop(columns_to_drop, axis=1)\n",
    "    df = df_removed.copy() \n",
    "    # roll_range = [1]  # Specify the desired lookback periods\n",
    "\n",
    "    roll_mean_features = pd.DataFrame()\n",
    "    roll_mean_features['will_be_1_anytime_next_week_lag'] = df_removed[target_name].shift(min(lookback_lag_target))\n",
    "    roll_mean_features = [roll_mean_features['will_be_1_anytime_next_week_lag'].rolling(roll).mean() for roll in roll_range]\n",
    "    roll_mean_columns = [f'roll_mean_{roll}' for roll in roll_range]\n",
    "    roll_mean_df = pd.concat(roll_mean_features, axis=1)\n",
    "    roll_mean_df.columns = roll_mean_columns\n",
    "\n",
    "    df = pd.concat([df, roll_mean_df], axis=1)\n",
    "    \n",
    "#      # Compute EMA\n",
    "#     ema_features = pd.DataFrame()\n",
    "#     ema_features['will_be_1_anytime_next_week_lag'] = df_removed[target_name].shift(min(lookback_lag_target))\n",
    "#     ema_features = [ema_features['will_be_1_anytime_next_week_lag'].ewm(span=roll).mean() for roll in roll_range]\n",
    "#     ema_columns = [f'ema_{roll}' for roll in roll_range]\n",
    "#     ema_df = pd.concat(ema_features, axis=1)\n",
    "#     ema_df.columns = ema_columns\n",
    "    \n",
    "#     df = pd.concat([df, ema_df], axis=1)\n",
    "    \n",
    "    # Add Fourier terms\n",
    "#     t = np.arange(len(df))\n",
    "#     frequencies = {\n",
    "#         'bi_weekly': 1/14,\n",
    "#         'weekly': 1/7,\n",
    "#         # 'monthly': 1/30,\n",
    "#         # 'yearly': 1/365\n",
    "#     }\n",
    "    \n",
    "#     for cycle, freq in frequencies.items():\n",
    "#         df[f'Fourier_{cycle}_sin'] = np.sin(2 * np.pi * freq * t)\n",
    "#         df[f'Fourier_{cycle}_cos'] = np.cos(2 * np.pi * freq * t)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "lookback_lag_target = range(window_gap+1,lookback+1)\n",
    "roll_range = range(window_gap+1,lookback-1)\n",
    "data_temp=data.copy()\n",
    "data_process = rolling(data_temp, lookback_lag_target, roll_range)\n",
    "data_process.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ebe00073-44fe-42c9-8520-2f93872d7ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SQLDATE</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>DayOfMonth</th>\n",
       "      <th>will_be_1_anytime_next_2_days</th>\n",
       "      <th>lag_1</th>\n",
       "      <th>lag_2</th>\n",
       "      <th>lag_3</th>\n",
       "      <th>lag_4</th>\n",
       "      <th>lag_5</th>\n",
       "      <th>...</th>\n",
       "      <th>lag_9</th>\n",
       "      <th>lag_10</th>\n",
       "      <th>roll_mean_1</th>\n",
       "      <th>roll_mean_2</th>\n",
       "      <th>roll_mean_3</th>\n",
       "      <th>roll_mean_4</th>\n",
       "      <th>roll_mean_5</th>\n",
       "      <th>roll_mean_6</th>\n",
       "      <th>roll_mean_7</th>\n",
       "      <th>roll_mean_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2015-03-01</td>\n",
       "      <td>2015</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2015-03-02</td>\n",
       "      <td>2015</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2015-03-03</td>\n",
       "      <td>2015</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2015-03-04</td>\n",
       "      <td>2015</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2015-03-05</td>\n",
       "      <td>2015</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       SQLDATE  Year  Month  DayOfMonth  will_be_1_anytime_next_2_days  lag_1  \\\n",
       "10  2015-03-01  2015      3           1                              0    0.0   \n",
       "11  2015-03-02  2015      3           2                              0    0.0   \n",
       "12  2015-03-03  2015      3           3                              0    0.0   \n",
       "13  2015-03-04  2015      3           4                              0    0.0   \n",
       "14  2015-03-05  2015      3           5                              0    0.0   \n",
       "\n",
       "    lag_2  lag_3  lag_4  lag_5  ...  lag_9  lag_10  roll_mean_1  roll_mean_2  \\\n",
       "10    0.0    0.0    0.0    0.0  ...    0.0     0.0          0.0          0.0   \n",
       "11    0.0    0.0    0.0    0.0  ...    0.0     0.0          0.0          0.0   \n",
       "12    0.0    0.0    0.0    0.0  ...    0.0     0.0          0.0          0.0   \n",
       "13    0.0    0.0    0.0    0.0  ...    0.0     0.0          0.0          0.0   \n",
       "14    0.0    0.0    0.0    0.0  ...    0.0     0.0          0.0          0.0   \n",
       "\n",
       "    roll_mean_3  roll_mean_4  roll_mean_5  roll_mean_6  roll_mean_7  \\\n",
       "10          0.0          0.0          0.0          0.0          0.0   \n",
       "11          0.0          0.0          0.0          0.0          0.0   \n",
       "12          0.0          0.0          0.0          0.0          0.0   \n",
       "13          0.0          0.0          0.0          0.0          0.0   \n",
       "14          0.0          0.0          0.0          0.0          0.0   \n",
       "\n",
       "    roll_mean_8  \n",
       "10          0.0  \n",
       "11          0.0  \n",
       "12          0.0  \n",
       "13          0.0  \n",
       "14          0.0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_process.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b14d07f9-b8fc-49aa-93b3-ba54e5e6c194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1747, 20, 22), (1747,))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_train_test_sequences_ordered(data, seq_length=20, target_column=None, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Create training and testing sequences from the data while maintaining the temporal order.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: DataFrame containing the data\n",
    "    - seq_length: Length of the sequences\n",
    "    - target_column: Name of the target column\n",
    "    - test_size: Proportion of the dataset to include in the test split\n",
    "    \n",
    "    Returns:\n",
    "    - X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    \n",
    "    if not target_column:\n",
    "        raise ValueError(\"Target column must be specified.\")\n",
    "    \n",
    "    # Get all features except target column\n",
    "    features = [col for col in data.columns if col != target_column]\n",
    "    \n",
    "    num_data = len(data) - seq_length + 1\n",
    "    num_features = len(features)\n",
    "    \n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(num_data):\n",
    "        seq = data[features].iloc[i:i+seq_length].values\n",
    "        target = data[target_column].iloc[i+seq_length-1]\n",
    "        \n",
    "        X.append(seq)\n",
    "        y.append(target)\n",
    "        \n",
    "#     split_index = int(len(X) * (1 - test_size))\n",
    "    \n",
    "#     X_train, X_test = X[:split_index], X[split_index:]\n",
    "#     y_train, y_test = y[:split_index], y[split_index:]\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "# Get training and testing sequences while maintaining the temporal order\n",
    "X, y = create_train_test_sequences_ordered(data_process, seq_length=20, target_column=target_name)\n",
    "X.shape, y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ace29f4-f9f1-40f0-8b3a-ebf8ec4ee1b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1397, 20, 22), (350, 20, 22), (1397,), (350,))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine the split index\n",
    "train_size = int(0.8 * len(X))\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "865b38d4-4353-4697-930f-3d4cc061382f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Convert data to PyTorch tensors (assuming X_train, y_train, X_test, y_test are already defined)\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m X_train_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     10\u001b[0m y_train_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(y_train, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m X_test_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X_test, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Check if CUDA is available and set our device to GPU if it is\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Convert data to PyTorch tensors (assuming X_train, y_train, X_test, y_test are already defined)\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "\n",
    "# Assuming the sequence length is equal to the number of time steps in each sample\n",
    "seq_length = X_train_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ca1162-85e4-466d-885d-10a1d356a125",
   "metadata": {
    "tags": []
   },
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9ac27a-4586-41d2-97f9-c160eeb7e39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, batch_size, output_dim=1, num_layers=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers)\n",
    "\n",
    "        # Define the output layer\n",
    "        self.linear = nn.Linear(self.hidden_dim, output_dim)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Initialize hidden state and cell state\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).to(device),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).to(device))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through LSTM layer\n",
    "        # lstm_out, self.hidden = self.lstm(x.view(len(x), seq_length, -1))\n",
    "        lstm_out, self.hidden = self.lstm(x.view(x.shape[0], seq_length, -1))\n",
    "        \n",
    "        # Only take the output from the final timestep\n",
    "        y_pred = self.linear(lstm_out[:, -1, :])\n",
    "        return y_pred\n",
    "\n",
    "# Define hyperparameters\n",
    "input_dim = X_train_tensor.shape[2]  # number of features\n",
    "hidden_dim = 256\n",
    "batch_size = 32  # we will use batch size of 1 for simplicity\n",
    "output_dim = 1\n",
    "num_layers = 2\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = LSTMModel(input_dim, hidden_dim, batch_size, output_dim, num_layers)\n",
    "model = model.to(device)\n",
    "imbalance_ratio = 8\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([imbalance_ratio]).to(device))\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3421cc17-c0bb-4839-ada9-1598aefc14a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu117 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(model, X_train_tensor, 'iris.onnx', input_names=[\"features\"], output_names=[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6322dd79-07f1-4f6d-adf8-02d19bd9431c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21bd612a-1422-41f3-9af5-2bc542164d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class ConvLSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, batch_size, output_dim=1, num_layers=1):\n",
    "        super(ConvLSTMModel, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Define the CNN layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_dim, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(128)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "        \n",
    "        self.conv3 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(256)\n",
    "        self.dropout3 = nn.Dropout(0.1)\n",
    "\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "\n",
    "        # Define the LSTM layer\n",
    "        # self.lstm = nn.LSTM(128, self.hidden_dim, self.num_layers)\n",
    "        self.lstm = nn.LSTM(256, self.hidden_dim, self.num_layers)\n",
    "        # self.lstm = nn.LSTM(64, self.hidden_dim, self.num_layers, dropout=0.2 if num_layers > 1 else 0)\n",
    "        # self.lstm = nn.LSTM(64, self.hidden_dim, self.num_layers)\n",
    "        # Define the output layer\n",
    "        self.linear = nn.Linear(self.hidden_dim, output_dim)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Initialize hidden state and cell state\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).to(device),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).to(device))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through CNN layers\n",
    "        x = self.conv1(x)\n",
    "        # x = self.batch_norm1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        # x = self.batch_norm2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        # x = self.batch_norm3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "\n",
    "        # Calculate new sequence length for LSTM\n",
    "        seq_length = x.shape[2]\n",
    "\n",
    "        # Forward pass through LSTM layer\n",
    "        # lstm_out, self.hidden = self.lstm(x.view(len(x), seq_length, -1))\n",
    "        lstm_out, self.hidden = self.lstm(x.view(x.shape[0], seq_length, -1))\n",
    "\n",
    "        # Only take the output from the final timestep\n",
    "        y_pred = self.linear(lstm_out[:, -1, :])\n",
    "        return y_pred\n",
    "\n",
    "# Define hyperparameters\n",
    "input_dim = X_train_tensor.shape[1]  # number of channels\n",
    "hidden_dim = 256\n",
    "batch_size = 32\n",
    "output_dim = 1\n",
    "num_layers = 2\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = ConvLSTMModel(input_dim, hidden_dim, batch_size, output_dim, num_layers)\n",
    "model = model.to(device)\n",
    "imbalance_ratio = 8\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([imbalance_ratio]).to(device))\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4cb15abe-131d-4c4b-a1d5-52a1eb485780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu117 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(model, X_train_tensor, 'iris.onnx', input_names=[\"features\"], output_names=[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa32657-2aae-4a55-9073-5137db2a06ae",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## BIDIRECTIONAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "52b7a1a7-41ed-405a-8b24-f5c9eeb5111c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirectLSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, batch_size, output_dim=1, num_layers=1):\n",
    "        super(ConvLSTMModel, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Define the CNN layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_dim, out_channels=64, kernel_size=7, padding=1)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=7, padding=1)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(128)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "        \n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "\n",
    "        # Define the LSTM layer with bidirectionality\n",
    "        self.lstm = nn.LSTM(128, self.hidden_dim, self.num_layers, \n",
    "                            dropout=0.2 if num_layers > 1 else 0, bidirectional=True)\n",
    "\n",
    "        # Adjust the input size for the linear layer to account for bidirectionality\n",
    "        self.linear = nn.Linear(2 * self.hidden_dim, output_dim)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Adjust the initialization for bidirectional hidden states\n",
    "        return (torch.zeros(2 * self.num_layers, self.batch_size, self.hidden_dim).to(device),\n",
    "                torch.zeros(2 * self.num_layers, self.batch_size, self.hidden_dim).to(device))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through CNN layers\n",
    "        x = self.conv1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Forward pass through LSTM layer\n",
    "        lstm_out, self.hidden = self.lstm(x.view(len(x), seq_length, -1))\n",
    "        \n",
    "        # Only take the output from the final timestep\n",
    "        # Remember to concatenate the outputs from both directions\n",
    "        y_pred = self.linear(torch.cat((lstm_out[:, -1, :self.hidden_dim], \n",
    "                                        lstm_out[:, 0, self.hidden_dim:]), 1))\n",
    "        return y_pred\n",
    "\n",
    "        # Define hyperparameters\n",
    "input_dim = X_train_tensor.shape[2]  # number of features\n",
    "hidden_dim = 256\n",
    "batch_size = 32  # we will use batch size of 1 for simplicity\n",
    "output_dim = 1\n",
    "num_layers = 2\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = BidirectLSTMModel(input_dim, hidden_dim, batch_size, output_dim, num_layers)\n",
    "model = model.to(device)\n",
    "imbalance_ratio = 8\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([imbalance_ratio]).to(device))\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de6266b-422c-4d3f-91b6-9392cd7b54ba",
   "metadata": {},
   "source": [
    "## Training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4927d12b-ad87-4711-bf07-c109f8bff01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "def compute_f1(model, X, y_true):\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X)\n",
    "        y_pred_binary = (torch.sigmoid(y_pred).cpu().numpy() > 0.5).astype(int)\n",
    "        cm = confusion_matrix(y_true, y_pred_binary)\n",
    "    return f1_score(y_true, y_pred_binary), cm\n",
    "\n",
    "# Retrain the model and compute f1 scores every 10 epochs\n",
    "num_epochs = 100\n",
    "f1_train_scores = []\n",
    "f1_test_scores = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    model.hidden = model.init_hidden()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    y_pred = model(X_train_tensor)\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = criterion(y_pred.view(-1), y_train_tensor)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Compute f1 scores every 10 epochs\n",
    "    if (epoch) % 20 == 0:\n",
    "        f1_train, _ = compute_f1(model, X_train_tensor, y_train)\n",
    "        f1_test, cm = compute_f1(model, X_test_tensor, y_test)\n",
    "        print(epoch, f1_test, f1_train, cm)\n",
    "        f1_train_scores.append(f1_train)\n",
    "        f1_test_scores.append(f1_test)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94042ea-25cb-4866-bb92-7f3c90c65c08",
   "metadata": {},
   "source": [
    "Permutation Importance for LSTM:\n",
    "The steps are as follows:\n",
    "\n",
    "Record the performance of the model with the original data (e.g., F1 score).\n",
    "\n",
    "For each feature:\n",
    "a. Permute the values of this feature across all samples.\n",
    "b. Record the performance of the model with this permuted data.\n",
    "c. The importance of this feature is the difference between the original performance and the permuted performance.\n",
    "\n",
    "Rank features based on their importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ef5f09-245a-483e-8af8-a8bf1e9fb6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def averaged_permutation_importance(model, X, y_true, metric, n_repeats=30):\n",
    "    \"\"\"\n",
    "    Compute averaged permutation importance for features in X using multiple permutations.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: trained LSTM model\n",
    "    - X: input data (tensor)\n",
    "    - y_true: true labels\n",
    "    - metric: function to compute performance metric\n",
    "    - n_repeats: number of times to repeat the permutation process for each feature\n",
    "    \n",
    "    Returns:\n",
    "    - importances: array of averaged importances for each feature\n",
    "    \"\"\"\n",
    "    # Convert tensors to numpy for permutation\n",
    "    X_np = X.cpu().numpy()\n",
    "    y_true_np = y_true.cpu().numpy()\n",
    "    \n",
    "    # Compute original metric\n",
    "    original_score, _ = metric(model, X, y_true_np)\n",
    "    \n",
    "    # Placeholder for importances\n",
    "    importances = np.zeros(X_np.shape[2])\n",
    "    \n",
    "    # Compute permutation importance\n",
    "    for i in range(X_np.shape[2]):\n",
    "        importance_accumulator = 0\n",
    "        for _ in range(n_repeats):\n",
    "            X_permuted = X_np.copy()\n",
    "            np.random.shuffle(X_permuted[:,:,i])\n",
    "            X_permuted_tensor = torch.tensor(X_permuted).to(device)\n",
    "            score_permuted, _ = metric(model, X_permuted_tensor, y_true_np)\n",
    "            importance_accumulator += original_score - score_permuted\n",
    "        # Average the accumulated importances\n",
    "        importances[i] = importance_accumulator / n_repeats\n",
    "    \n",
    "    return importances\n",
    "\n",
    "# Assuming target_name is defined and is the name of the target variable in data_process\n",
    "# Exclude target_name from the DataFrame to get feature names\n",
    "feature_names = data_process.drop(columns=[target_name]).columns.tolist()\n",
    "\n",
    "# Compute averaged permutation importances for the trained model using the test data\n",
    "avg_importances = averaged_permutation_importance(model, X_test_tensor, y_test_tensor, compute_f1)\n",
    "\n",
    "# Plot the averaged importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(avg_importances)), avg_importances)\n",
    "plt.xlabel('Features')\n",
    "plt.xticks(range(len(avg_importances)), feature_names, rotation=45)\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Averaged Permutation Importance of LSTM Features')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1f692e-87d2-4bb2-8fb7-e9d17837c197",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m108"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
